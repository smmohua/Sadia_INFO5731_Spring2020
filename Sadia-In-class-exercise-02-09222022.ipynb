{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The second In-class-exercise (09/22/2022, 40 points in total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The purpose of this exercise is to understand users' information needs, then collect data from different sources for analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 1 (10 points): Describe an interesting research question (or practical question) you have in mind, what kind of data should be collected to answer the question(s)? How many data needed for the analysis? The detail steps for collecting and save the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nPlease write you answer here:\\n\\nDomain problem: \\n. My domain problem is to analyse the price data that obtained from the ebay of colection og women tops.\\n\\nResearch question:\\n. At what price would you begin to think the {product/service} is so inexpensive \\n\\nData Required to answer the questions:\\nThe following are the attributes of the required data to answer the research question\\n. price of the product\\n\\nWe can gather 20 page samples from the following link: https://www.amazon.com/\\n\\n\\nSteps for collecting and saving the data:\\n. We use web scraping technique to scrape the data.\\n. We use BeautifulSoup library to parse the Densho Digital Repository.\\n. There are 1000 pages of women tops.\\n. We parse through each page and go to the women tops link.\\n. We parse the Segment Body-> span elements in the html code and then store the text data in a dictionary format.\\n. The dictionary is the prepared data for our research.\\n\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Your answer here (no code for this question, write down your answer as detail as possible for the above questions):\n",
    "\n",
    "'''\n",
    "Please write you answer here:\n",
    "\n",
    "Domain problem: \n",
    ". My domain problem is to analyse the price data that obtained from the ebay of colection og women tops.\n",
    "\n",
    "Research question:\n",
    ". At what price would you begin to think the {product/service} is so inexpensive \n",
    "\n",
    "Data Required to answer the questions:\n",
    "The following are the attributes of the required data to answer the research question\n",
    ". price of the product\n",
    "\n",
    "We can gather 20 page samples from the following link: https://www.amazon.com/\n",
    "\n",
    "\n",
    "Steps for collecting and saving the data:\n",
    ". We use web scraping technique to scrape the data.\n",
    ". We use BeautifulSoup library to parse the Densho Digital Repository.\n",
    ". There are 1000 pages of women tops.\n",
    ". We parse through each page and go to the women tops link.\n",
    ". We parse the Segment Body-> span elements in the html code and then store the text data in a dictionary format.\n",
    ". The dictionary is the prepared data for our research.\n",
    "\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 2 (10 points): Write python code to collect 1000 data samples you discussed above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Price\n",
      "0    350.\n",
      "1    220.\n",
      "2    280.\n",
      "3    142.\n",
      "4    295.\n",
      "..    ...\n",
      "907  162.\n",
      "908  225.\n",
      "909  485.\n",
      "910  625.\n",
      "911   86.\n",
      "\n",
      "[912 rows x 1 columns]\n"
     ]
    }
   ],
   "source": [
    "# You code here (Please add comments in the code):\n",
    "\n",
    "# importing the libraries...\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.request import Request, urlopen\n",
    "from urllib.error import HTTPError\n",
    "import json\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "\n",
    "# url of the densho digital repository to scrape the data\n",
    "main_url = \"https://www.amazon.com/s?i=fashion&bbn=23370962011&rh=n%3A7141123011%2Cn%3A23370962011%2Cn%3A7147440011%2Cn%3A1040660%2Cn%3A1045024%2Cp_85%3A2470955011&s=date-desc-rank&dc&ds=v1%3ApftFVJZe9iCRK6ECM1qkAx5HsC8yTxqkt22TjenEbSw&content-id=amzn1.sym.353f2924-e405-4b4f-bf96-d5b798e102ba%3Aamzn1.sym.353f2924-e405-4b4f-bf96-d5b798e102ba&pd_rd_r=fc410f66-b8f1-4900-a1e1-6eaf7e395a48&pd_rd_w=CUr7E&pd_rd_wg=UsmKm&pf_rd_i=7147440011&pf_rd_m=ATVPDKIKX0DER&pf_rd_p=353f2924-e405-4b4f-bf96-d5b798e102ba&pf_rd_r=15QAGSSYGH2302FRNQPY&pf_rd_s=merchandised-search-{}&pf_rd_t=101&qid=1664146062&rnid=1040660&wi=l61aaov1_0&ref=slsr_premiumfall2022category_d_t_m_vn_l61aaov1_0\"\n",
    "df = pd.DataFrame()\n",
    "price_list_dict = {\"Price\": []}\n",
    "for page_num in range(1, 20):\n",
    "    page_num = page_num + 9\n",
    "    # we create the data soup of the main url page html elements.\n",
    "    link1 = Request(main_url.format(page_num), headers={'User-Agent': 'Mozilla/5.0'})\n",
    "    url1 = urlopen(link1)\n",
    "    data1 = url1.read()\n",
    "    data1_soup = BeautifulSoup(data1)\n",
    "    # print(data1_soup)\n",
    "    \n",
    "    prices = data1_soup.find_all(\"span\", {\"class\": \"a-price-whole\"})\n",
    "    for price in prices:\n",
    "        price_list_dict['Price'].append(price.text)\n",
    "\n",
    "\n",
    "df = pd.DataFrame(price_list_dict)    \n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 3 (10 points): Write python code to collect 1000 articles from Google Scholar (https://scholar.google.com/), Microsoft Academic (https://academic.microsoft.com/home), or CiteSeerX (https://citeseerx.ist.psu.edu/index), or Semantic Scholar (https://www.semanticscholar.org/), or ACM Digital Libraries (https://dl.acm.org/) with the keyword \"information retrieval\". The articles should be published in the last 10 years (2012-2022).\n",
    "\n",
    "The following information of the article needs to be collected:\n",
    "\n",
    "(1) Title\n",
    "\n",
    "(2) Venue/journal/conference being published\n",
    "\n",
    "(3) Year\n",
    "\n",
    "(4) Authors\n",
    "\n",
    "(5) Abstract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                Titles  \\\n",
      "0          [BOOK][B] Information retrieval interaction   \n",
      "1    [PDF][PDF] Evaluation of evaluation in informa...   \n",
      "2                                Information retrieval   \n",
      "3           [CITATION][C] Modern information retrieval   \n",
      "4    A definition of relevance for information retr...   \n",
      "..                                                 ...   \n",
      "185  Adarank: a boosting algorithm for information ...   \n",
      "186    [HTML][HTML] Geographical information retrieval   \n",
      "187             Terrier information retrieval platform   \n",
      "188  [BOOK][B] Information retrieval: data structur...   \n",
      "189                  Distributed information retrieval   \n",
      "\n",
      "                                       Author_and_Year  \\\n",
      "0                 P Ingwersen - 1992 - oarklibrary.com   \n",
      "1    T Saracevic - … on Research and development in...   \n",
      "2            Q Mei, DR Radev - 2014 - academic.oup.com   \n",
      "3          BY Ricardo - 1999 - Pearson Education India   \n",
      "4    WS Cooper - Information storage and retrieval,...   \n",
      "..                                                 ...   \n",
      "185  J Xu, H Li - … on Research and development in ...   \n",
      "186  CB Jones, RS Purves - … Journal of Geographica...   \n",
      "187  I Ounis, G Amati, V Plachouras, B He… - … on I...   \n",
      "188       WB Frakes, R Baeza-Yates - 1992 - dl.acm.org   \n",
      "189  J Callan - Advances in information retrieval, ...   \n",
      "\n",
      "                                              Abstract  \n",
      "0    … Information retrieval covers the problems re...  \n",
      "1    ABSTRACT Evaluation is a major force in resear...  \n",
      "2    Information Retrieval | The Oxford Handbook of...  \n",
      "3                                                       \n",
      "4    … The purpose of this paper, then, is to propo...  \n",
      "..                                                 ...  \n",
      "185  In this paper we address the issue of learning...  \n",
      "186  … geographic relevance and the non‐retrieval o...  \n",
      "187  Terrier is a modular platform for the rapid de...  \n",
      "188  … covering the major information retrieval alg...  \n",
      "189  A multi-database model of distributed informat...  \n",
      "\n",
      "[190 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "# You code here (Please add comments in the code):\n",
    "\n",
    "# importing the libraries...\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.request import Request, urlopen\n",
    "from urllib.error import HTTPError\n",
    "import json\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "\n",
    "# url of the densho digital repository to scrape the data\n",
    "main_url = \"https://scholar.google.com/scholar?start={}&q=information+retrieval&hl=en&as_sdt=0,44\"\n",
    "df = pd.DataFrame()\n",
    "res_dict = {\"Titles\": [], \"Author_and_Year\": [], \"Abstract\": []}\n",
    "for page_num in range(1, 20):\n",
    "    # we create the data soup of the main url page html elements.\n",
    "    page_num=page_num + 9\n",
    "    link1 = Request(main_url.format(page_num), headers={'User-Agent': 'Mozilla/5.0'})\n",
    "    url1 = urlopen(link1)\n",
    "    data1 = url1.read()\n",
    "    data1_soup = BeautifulSoup(data1)\n",
    "    \n",
    "    title_list = data1_soup.find_all(\"h3\", {\"class\": \"gs_rt\"})\n",
    "    author_year_list = data1_soup.find_all(\"div\", {\"class\": \"gs_a\"})\n",
    "    abstract_list = data1_soup.find_all(\"div\", {\"class\": \"gs_rs\"})\n",
    "    \n",
    "    for title in title_list:\n",
    "        res_dict['Titles'].append(title.text)\n",
    "    for author_year in author_year_list:\n",
    "        res_dict['Author_and_Year'].append(author_year.text)\n",
    "    for abstract in abstract_list:\n",
    "        res_dict['Abstract'].append(abstract.text)\n",
    "\n",
    "\n",
    "df = pd.DataFrame(res_dict)    \n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 4 (10 points): Write python code to collect 1000 posts from Twitter, or Facebook, or Instagram. You can either use hashtags, keywords, user_name, user_id, or other information to collect the data. \n",
    "\n",
    "The following information needs to be collected:\n",
    "\n",
    "(1) User_name\n",
    "\n",
    "(2) Posted time\n",
    "\n",
    "(3) Text "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "Unauthorized",
     "evalue": "401 Unauthorized\n89 - Invalid or expired token.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnauthorized\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[0;32mIn [4]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m#Use csv Writer\u001b[39;00m\n\u001b[1;32m     18\u001b[0m csvWriter \u001b[38;5;241m=\u001b[39m csv\u001b[38;5;241m.\u001b[39mwriter(csvFile)\n\u001b[0;32m---> 20\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m tweets \u001b[38;5;129;01min\u001b[39;00m \u001b[43mapi\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msearch_tweets\u001b[49m\u001b[43m(\u001b[49m\u001b[43mq\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m#unitedAIRLINES\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mcount\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mlang\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43men\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[43m                           \u001b[49m\u001b[43muntil\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m2017-04-03\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m     23\u001b[0m     \u001b[38;5;28mprint\u001b[39m (tweet\u001b[38;5;241m.\u001b[39mcreated_at, tweet\u001b[38;5;241m.\u001b[39mtext)\n\u001b[1;32m     24\u001b[0m     csvWriter\u001b[38;5;241m.\u001b[39mwriterow([tweet\u001b[38;5;241m.\u001b[39mcreated_at, tweet\u001b[38;5;241m.\u001b[39mtext\u001b[38;5;241m.\u001b[39mencode(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m)])\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/tweepy/api.py:33\u001b[0m, in \u001b[0;36mpagination.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(method)\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m---> 33\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmethod\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/tweepy/api.py:46\u001b[0m, in \u001b[0;36mpayload.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     44\u001b[0m kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpayload_list\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m payload_list\n\u001b[1;32m     45\u001b[0m kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpayload_type\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m payload_type\n\u001b[0;32m---> 46\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmethod\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/tweepy/api.py:1303\u001b[0m, in \u001b[0;36mAPI.search_tweets\u001b[0;34m(self, q, **kwargs)\u001b[0m\n\u001b[1;32m   1209\u001b[0m \u001b[38;5;129m@pagination\u001b[39m(mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m   1210\u001b[0m \u001b[38;5;129m@payload\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msearch_results\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m   1211\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msearch_tweets\u001b[39m(\u001b[38;5;28mself\u001b[39m, q, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m   1212\u001b[0m     \u001b[38;5;124;03m\"\"\"search_tweets(q, *, geocode, lang, locale, result_type, count, \\\u001b[39;00m\n\u001b[1;32m   1213\u001b[0m \u001b[38;5;124;03m                     until, since_id, max_id, include_entities)\u001b[39;00m\n\u001b[1;32m   1214\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1301\u001b[0m \u001b[38;5;124;03m    .. _Twitter's documentation on the standard search API: https://developer.twitter.com/en/docs/twitter-api/v1/tweets/search/overview\u001b[39;00m\n\u001b[1;32m   1302\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1303\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1304\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mGET\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msearch/tweets\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mendpoint_parameters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1305\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mq\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mgeocode\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlang\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlocale\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mresult_type\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcount\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1306\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43muntil\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msince_id\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmax_id\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43minclude_entities\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\n\u001b[1;32m   1307\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mq\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m   1308\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/tweepy/api.py:257\u001b[0m, in \u001b[0;36mAPI.request\u001b[0;34m(self, method, endpoint, endpoint_parameters, params, headers, json_payload, parser, payload_list, payload_type, post_data, files, require_auth, return_cursors, upload_api, use_cache, **kwargs)\u001b[0m\n\u001b[1;32m    255\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m BadRequest(resp)\n\u001b[1;32m    256\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m resp\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m401\u001b[39m:\n\u001b[0;32m--> 257\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Unauthorized(resp)\n\u001b[1;32m    258\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m resp\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m403\u001b[39m:\n\u001b[1;32m    259\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Forbidden(resp)\n",
      "\u001b[0;31mUnauthorized\u001b[0m: 401 Unauthorized\n89 - Invalid or expired token."
     ]
    }
   ],
   "source": [
    "# You code here (Please add comments in the code):\n",
    "import tweepy\n",
    "import csv\n",
    "import pandas as pd\n",
    "####input your credentials here\n",
    "consumer_key = 'u7L11nR7HN85dn1qnTFO1cegb'\n",
    "consumer_secret = 'QN1JrEmit2To46ZcwWAT4aI5QGWZXWRDDUPnMCWV5M66SFc8wT'\n",
    "access_token = '1144377060036620294-BSEicX3zH7hIhksbNZV9mrWFwa07co'\n",
    "access_token_secret = 'gxWMOodDq1nQAjix9mHEOUSAtgE7XH5ctHInm0XRs1Jce'\n",
    "\n",
    "auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
    "auth.set_access_token(access_token, access_token_secret)\n",
    "api = tweepy.API(auth,wait_on_rate_limit=True)\n",
    "#####United Airlines\n",
    "# Open/Create a file to append data\n",
    "csvFile = open('UnitedAirlines.csv', 'a')\n",
    "#Use csv Writer\n",
    "csvWriter = csv.writer(csvFile)\n",
    "\n",
    "for tweets in api.search_tweets(q=\"#unitedAIRLINES\",count=100,\n",
    "                           lang=\"en\",\n",
    "                           until=\"2017-04-03\").items():\n",
    "    print (tweet.created_at, tweet.text)\n",
    "    csvWriter.writerow([tweet.created_at, tweet.text.encode('utf-8')])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
